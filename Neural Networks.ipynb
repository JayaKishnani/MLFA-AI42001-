{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef736818",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Done by:\n",
    "- Saumili Jana: 20ME10075\n",
    "- Ushasi Das: 20EC10086\n",
    "- Jaya Kishnani: 20EC30020\n",
    "- Karthikeyan.R: 20EC30024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9df33",
   "metadata": {},
   "source": [
    "## Part 1: Effect of NN Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f578b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34fa9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4188cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist.load_data?\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ddf037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd731154b20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9289b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the values to improve the accuracy of the model\n",
    "x_train=x_train.astype(\"float32\")\n",
    "y_train=y_train.astype(\"float32\")\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d3ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "flatxtrain = x_train.reshape(len(x_train), 28*28)\n",
    "flatxtest = x_test.reshape(len(x_test), 28*28)\n",
    "#print(flatxtrain[1][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e82800",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.3                                              #dropout\n",
    "initializer = tf.keras.initializers.GlorotNormal()   #Xavier's initialization\n",
    "N = 100000                                           # Parameters\n",
    "L = 10                                               # No of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf521c",
   "metadata": {},
   "source": [
    "### Building the 3 neural nets\n",
    "#### A)UniformNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6517324",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim=87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e4128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform net\n",
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.InputLayer((784)))\n",
    "\n",
    "model1.add(layers.Dense(50, activation='relu', name='first_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim,activation='relu', name='second_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim,activation='relu', name='third_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim,activation='relu', name='fourth_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim,activation='relu', name='fifth_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim,activation='relu', name='sixth_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim, activation='relu', name='seventh_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim, activation='relu', name='eighth_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(ndim, activation='relu', name='ninth_hidden_layer',kernel_initializer=initializer))\n",
    "model1.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model1.add(layers.Dropout(d))\n",
    "\n",
    "model1.add(layers.Dense(10, activation='softmax', name='output_layer',kernel_initializer=initializer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b28e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " first_hidden_layer (Dense)  (None, 50)                39250     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 50)               200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " second_hidden_layer (Dense)  (None, 87)               4437      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " third_hidden_layer (Dense)  (None, 87)                7656      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " fourth_hidden_layer (Dense)  (None, 87)               7656      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " fifth_hidden_layer (Dense)  (None, 87)                7656      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " sixth_hidden_layer (Dense)  (None, 87)                7656      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " seventh_hidden_layer (Dense  (None, 87)               7656      \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " eighth_hidden_layer (Dense)  (None, 87)               7656      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " ninth_hidden_layer (Dense)  (None, 87)                7656      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 87)               348       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 87)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                880       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,143\n",
      "Trainable params: 99,651\n",
      "Non-trainable params: 1,492\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd82b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 6s 23ms/step - loss: 2.9867 - accuracy: 0.1054\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 2s 19ms/step - loss: 2.7852 - accuracy: 0.1179\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.6129 - accuracy: 0.1324\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.4507 - accuracy: 0.1564\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.3253 - accuracy: 0.1816\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 2.1994 - accuracy: 0.2121\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.0729 - accuracy: 0.2451\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.9785 - accuracy: 0.2754\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.8775 - accuracy: 0.3076\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.7860 - accuracy: 0.3347\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.6978 - accuracy: 0.3647\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.6212 - accuracy: 0.3873\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 1.5547 - accuracy: 0.4131\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.4911 - accuracy: 0.4317\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 1.4356 - accuracy: 0.4527\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 1.3807 - accuracy: 0.4728\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.3317 - accuracy: 0.4943\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 1.2742 - accuracy: 0.5171\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 1.2281 - accuracy: 0.5418\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.1752 - accuracy: 0.5614\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 1.1283 - accuracy: 0.5840\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 1.0749 - accuracy: 0.6113\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.0205 - accuracy: 0.6352\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.9820 - accuracy: 0.6540\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.9409 - accuracy: 0.6739\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.8987 - accuracy: 0.6943\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.8652 - accuracy: 0.7077\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.8268 - accuracy: 0.7256\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.8030 - accuracy: 0.7397\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.7715 - accuracy: 0.7541\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.7448 - accuracy: 0.7660\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.7243 - accuracy: 0.7779\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6857 - accuracy: 0.7940\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6746 - accuracy: 0.7999\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6510 - accuracy: 0.8110\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.6267 - accuracy: 0.8201\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6082 - accuracy: 0.8275\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5877 - accuracy: 0.8353\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5760 - accuracy: 0.8402\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.5570 - accuracy: 0.8468\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.5411 - accuracy: 0.8554\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.5257 - accuracy: 0.8579\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.5137 - accuracy: 0.8624\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.5005 - accuracy: 0.8689\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4865 - accuracy: 0.8713\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4773 - accuracy: 0.8748\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4660 - accuracy: 0.8788\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4595 - accuracy: 0.8818\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4514 - accuracy: 0.8828\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.4365 - accuracy: 0.8876\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.4358 - accuracy: 0.8902\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4262 - accuracy: 0.8936\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4184 - accuracy: 0.8953\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4122 - accuracy: 0.8964\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.4065 - accuracy: 0.8971\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4043 - accuracy: 0.8994\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.3939 - accuracy: 0.9011\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.3942 - accuracy: 0.9025\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.3895 - accuracy: 0.9030\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.3783 - accuracy: 0.9057\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.3763 - accuracy: 0.9070\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3726 - accuracy: 0.9085\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3697 - accuracy: 0.9095\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.3621 - accuracy: 0.9112\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.3605 - accuracy: 0.9129\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.3533 - accuracy: 0.9132\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3513 - accuracy: 0.9139\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.3455 - accuracy: 0.9169\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3468 - accuracy: 0.9156\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.3387 - accuracy: 0.9176\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 4s 32ms/step - loss: 0.3379 - accuracy: 0.9174\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.3346 - accuracy: 0.9183\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.3323 - accuracy: 0.9193\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.3262 - accuracy: 0.9205\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3193 - accuracy: 0.9231\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3240 - accuracy: 0.9218\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.3193 - accuracy: 0.9234\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.3184 - accuracy: 0.9236\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.3161 - accuracy: 0.9240\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 3s 22ms/step - loss: 0.3107 - accuracy: 0.9251\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.3108 - accuracy: 0.9249\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.3046 - accuracy: 0.9257\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.3061 - accuracy: 0.9261\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.3030 - accuracy: 0.9266\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.2994 - accuracy: 0.9279\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.3010 - accuracy: 0.9281\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.3014 - accuracy: 0.9273\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.2968 - accuracy: 0.9291\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.2890 - accuracy: 0.9310\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.2871 - accuracy: 0.9314\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.2919 - accuracy: 0.9302\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.2871 - accuracy: 0.9306\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.2874 - accuracy: 0.9317\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.2823 - accuracy: 0.9327\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 4s 38ms/step - loss: 0.2814 - accuracy: 0.9322\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.2788 - accuracy: 0.9327\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.2801 - accuracy: 0.9328\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.2745 - accuracy: 0.9324\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.2768 - accuracy: 0.9329\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.2736 - accuracy: 0.9337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd731b1b1f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),                  # Cross-Entropy loss function\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001),            #Adam optimizer\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "model1.fit(flatxtrain, y_train, batch_size =512, epochs =100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d9beef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9547\n",
      "accuracy = 95.46999931335449 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model on test set\n",
    "(h,acc1)=model1.evaluate(flatxtest, y_test)\n",
    "print('accuracy =', acc1*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c5c38",
   "metadata": {},
   "source": [
    "#### B) PyramidNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c691aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1=189\n",
    "v=2**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f05d695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyramid net\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.InputLayer((784)))\n",
    "\n",
    "model2.add(layers.Dense(50, activation='relu', name='first_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(n1,activation='relu', name='second_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/v),activation='relu', name='third_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**2)),activation='relu', name='fourth_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**3)),activation='relu', name='fifth_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**4)),activation='relu', name='sixth_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**5)), activation='relu', name='seventh_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**6)), activation='relu', name='eighth_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(int(n1/(v**7)), activation='relu', name='ninth_hidden_layer',kernel_initializer=initializer))\n",
    "model2.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model2.add(layers.Dropout(d))\n",
    "\n",
    "model2.add(layers.Dense(10, activation='softmax', name='output_layer',kernel_initializer=initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9f9e6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " first_hidden_layer (Dense)  (None, 50)                39250     \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 50)               200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " second_hidden_layer (Dense)  (None, 189)              9639      \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 189)              756       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 189)               0         \n",
      "                                                                 \n",
      " third_hidden_layer (Dense)  (None, 133)               25270     \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 133)              532       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 133)               0         \n",
      "                                                                 \n",
      " fourth_hidden_layer (Dense)  (None, 94)               12596     \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 94)               376       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 94)                0         \n",
      "                                                                 \n",
      " fifth_hidden_layer (Dense)  (None, 66)                6270      \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 66)               264       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 66)                0         \n",
      "                                                                 \n",
      " sixth_hidden_layer (Dense)  (None, 47)                3149      \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 47)               188       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 47)                0         \n",
      "                                                                 \n",
      " seventh_hidden_layer (Dense  (None, 33)               1584      \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 33)               132       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 33)                0         \n",
      "                                                                 \n",
      " eighth_hidden_layer (Dense)  (None, 23)               782       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 23)               92        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 23)                0         \n",
      "                                                                 \n",
      " ninth_hidden_layer (Dense)  (None, 16)                384       \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,698\n",
      "Trainable params: 100,396\n",
      "Non-trainable params: 1,302\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4476c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 5s 22ms/step - loss: 2.8945 - accuracy: 0.1085\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 2.7410 - accuracy: 0.1215\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 2.6026 - accuracy: 0.1329\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 2.4766 - accuracy: 0.1464\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 2.3752 - accuracy: 0.1605\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 2.2922 - accuracy: 0.1750\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.2229 - accuracy: 0.1890\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 2.1662 - accuracy: 0.2047\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 2.1156 - accuracy: 0.2219\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 2.0697 - accuracy: 0.2400\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 2.0201 - accuracy: 0.2594\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 1.9752 - accuracy: 0.2795\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.9213 - accuracy: 0.3017\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 1.8724 - accuracy: 0.3256\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.8135 - accuracy: 0.3534\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.7534 - accuracy: 0.3788\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.6926 - accuracy: 0.4053\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.6340 - accuracy: 0.4321\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.5709 - accuracy: 0.4590\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.5061 - accuracy: 0.4880\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.4453 - accuracy: 0.5133\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 1.3926 - accuracy: 0.5338\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 4s 32ms/step - loss: 1.3403 - accuracy: 0.5548\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 4s 32ms/step - loss: 1.2878 - accuracy: 0.5742\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.2411 - accuracy: 0.5890\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.1959 - accuracy: 0.6033\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 1.1580 - accuracy: 0.6185\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 1.1215 - accuracy: 0.6285\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.0808 - accuracy: 0.6411\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.0446 - accuracy: 0.6556\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 4s 36ms/step - loss: 1.0163 - accuracy: 0.6619\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.9892 - accuracy: 0.6717\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.9646 - accuracy: 0.6801\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.9381 - accuracy: 0.6895\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.9121 - accuracy: 0.7007\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.8926 - accuracy: 0.7054\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.8657 - accuracy: 0.7129\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.8465 - accuracy: 0.7229\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.8267 - accuracy: 0.7308\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.8104 - accuracy: 0.7372\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7930 - accuracy: 0.7405\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.7720 - accuracy: 0.7521\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.7649 - accuracy: 0.7528\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.7460 - accuracy: 0.7598\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.7342 - accuracy: 0.7653\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.7179 - accuracy: 0.7708\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.7065 - accuracy: 0.7781\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.6912 - accuracy: 0.7827\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6815 - accuracy: 0.7879\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6651 - accuracy: 0.7928\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6540 - accuracy: 0.7987\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6407 - accuracy: 0.8052\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6280 - accuracy: 0.8121\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.6205 - accuracy: 0.8151\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.6101 - accuracy: 0.8205\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.6009 - accuracy: 0.8259\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.5879 - accuracy: 0.8324\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.5767 - accuracy: 0.8374\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.5658 - accuracy: 0.8437\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5578 - accuracy: 0.8483\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5430 - accuracy: 0.8548\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5431 - accuracy: 0.8566\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.5309 - accuracy: 0.8623\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.5277 - accuracy: 0.8641\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.5153 - accuracy: 0.8696\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.5060 - accuracy: 0.8728\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4955 - accuracy: 0.8773\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4958 - accuracy: 0.8756\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4820 - accuracy: 0.8815\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4786 - accuracy: 0.8838\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4724 - accuracy: 0.8862\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4658 - accuracy: 0.8882\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4597 - accuracy: 0.8905\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4524 - accuracy: 0.8916\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4535 - accuracy: 0.8949\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4450 - accuracy: 0.8969\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 0.4501 - accuracy: 0.8951\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4371 - accuracy: 0.9009\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.4338 - accuracy: 0.8997\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 3s 30ms/step - loss: 0.4329 - accuracy: 0.9024\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.4221 - accuracy: 0.9040\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4217 - accuracy: 0.9023\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.4167 - accuracy: 0.9065\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.4092 - accuracy: 0.9080\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.4026 - accuracy: 0.9107\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.3987 - accuracy: 0.9113\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4088 - accuracy: 0.9104\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.3962 - accuracy: 0.9122\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.3917 - accuracy: 0.9148\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.3861 - accuracy: 0.9149\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 4s 34ms/step - loss: 0.3880 - accuracy: 0.9155\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.3828 - accuracy: 0.9176\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 0.3832 - accuracy: 0.9167\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.3811 - accuracy: 0.9178\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3795 - accuracy: 0.9183\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.3756 - accuracy: 0.9202\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3738 - accuracy: 0.9198\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3687 - accuracy: 0.9219\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.3656 - accuracy: 0.9212\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.3618 - accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd73610e670>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),                  # Cross-Entropy loss function\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001),            #Adam optimizer\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "model2.fit(flatxtrain, y_train, batch_size =512, epochs =100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d49d8bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2174 - accuracy: 0.9572\n",
      "accuracy = 95.71999907493591 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model on test set\n",
    "(h2,acc2)= model2.evaluate(flatxtest, y_test)\n",
    "print('accuracy =', acc2*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe3c29",
   "metadata": {},
   "source": [
    "#### C) InvPyramidNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5db9b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2=18\n",
    "v=2**(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3022d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse Pyramid net\n",
    "model3 = keras.Sequential()\n",
    "model3.add(keras.layers.InputLayer((784)))\n",
    "\n",
    "model3.add(layers.Dense(50, activation='relu', name='first_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(n2,activation='relu', name='second_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/v),activation='relu', name='third_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**2)),activation='relu', name='fourth_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**3)),activation='relu', name='fifth_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**4)),activation='relu', name='sixth_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**5)), activation='relu', name='seventh_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**6)), activation='relu', name='eighth_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(int(n2/(v**7)), activation='relu', name='ninth_hidden_layer',kernel_initializer=initializer))\n",
    "model3.add(layers.BatchNormalization())                                                                      # Batch normalization    \n",
    "model3.add(layers.Dropout(d))\n",
    "\n",
    "model3.add(layers.Dense(10, activation='softmax', name='output_layer',kernel_initializer=initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f3745bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 5s 22ms/step - loss: 3.0332 - accuracy: 0.1070\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 2.7380 - accuracy: 0.1254\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 2.5731 - accuracy: 0.1408\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 2.4384 - accuracy: 0.1605\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 2.3357 - accuracy: 0.1798\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 2.2467 - accuracy: 0.1987\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 2.1690 - accuracy: 0.2206\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 2.1002 - accuracy: 0.2415\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 2.0151 - accuracy: 0.2689\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.9414 - accuracy: 0.2934\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.8624 - accuracy: 0.3183\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.7945 - accuracy: 0.3415\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.7243 - accuracy: 0.3645\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.6663 - accuracy: 0.3817\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.6014 - accuracy: 0.4036\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 4s 33ms/step - loss: 1.5572 - accuracy: 0.4178\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.5071 - accuracy: 0.4333\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 3s 29ms/step - loss: 1.4598 - accuracy: 0.4428\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 1.4128 - accuracy: 0.4594\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.3732 - accuracy: 0.4701\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.3403 - accuracy: 0.4836\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.3062 - accuracy: 0.4946\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 1.2724 - accuracy: 0.5055\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.2430 - accuracy: 0.5167\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 1.2185 - accuracy: 0.5240\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 1.1918 - accuracy: 0.5295\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.1639 - accuracy: 0.5418\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 1.1434 - accuracy: 0.5486\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 1.1260 - accuracy: 0.5555\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 1.1108 - accuracy: 0.5624\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 1.0882 - accuracy: 0.5710\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 1.0694 - accuracy: 0.5806\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 1.0500 - accuracy: 0.5890\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 1.0324 - accuracy: 0.5972\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 1.0177 - accuracy: 0.6056\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 1.0031 - accuracy: 0.6150\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.9838 - accuracy: 0.6258\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 0.9618 - accuracy: 0.6377\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.9416 - accuracy: 0.6466\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.9345 - accuracy: 0.6538\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.9137 - accuracy: 0.6633\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.8960 - accuracy: 0.6736\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.8780 - accuracy: 0.6876\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.8651 - accuracy: 0.6911\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.8534 - accuracy: 0.6985\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.8308 - accuracy: 0.7101\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.8191 - accuracy: 0.7153\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.7977 - accuracy: 0.7248\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.7830 - accuracy: 0.7332\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.7669 - accuracy: 0.7410\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7597 - accuracy: 0.7445\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.7432 - accuracy: 0.7536\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7319 - accuracy: 0.7587\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7173 - accuracy: 0.7634\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7120 - accuracy: 0.7678\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.7000 - accuracy: 0.7718\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6799 - accuracy: 0.7823\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.6732 - accuracy: 0.7861\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6565 - accuracy: 0.7914\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.6498 - accuracy: 0.7957\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.6474 - accuracy: 0.7985\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6311 - accuracy: 0.8050\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6277 - accuracy: 0.8074\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.6171 - accuracy: 0.8105\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6114 - accuracy: 0.8134\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.6026 - accuracy: 0.8169\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5935 - accuracy: 0.8204\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5802 - accuracy: 0.8249\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.5737 - accuracy: 0.8279\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5661 - accuracy: 0.8312\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.5661 - accuracy: 0.8321\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.5536 - accuracy: 0.8369\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.5511 - accuracy: 0.8398\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.5485 - accuracy: 0.8393\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.5378 - accuracy: 0.8443\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.5330 - accuracy: 0.8446\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.5324 - accuracy: 0.8462\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.5155 - accuracy: 0.8524\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.5141 - accuracy: 0.8511\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 3s 26ms/step - loss: 0.5121 - accuracy: 0.8537\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.5070 - accuracy: 0.8570\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 4s 31ms/step - loss: 0.5032 - accuracy: 0.8576\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.5018 - accuracy: 0.8588\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.4896 - accuracy: 0.8619\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.4816 - accuracy: 0.8644\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.4773 - accuracy: 0.8659\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.4817 - accuracy: 0.8668\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 3s 27ms/step - loss: 0.4701 - accuracy: 0.8700\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.4710 - accuracy: 0.8696\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 4s 30ms/step - loss: 0.4637 - accuracy: 0.8715\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.4613 - accuracy: 0.8737\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4616 - accuracy: 0.8726\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4599 - accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 3s 28ms/step - loss: 0.4561 - accuracy: 0.8751\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4452 - accuracy: 0.8782\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.4450 - accuracy: 0.8775\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4446 - accuracy: 0.8787\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.4472 - accuracy: 0.8783\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4334 - accuracy: 0.8831\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.4307 - accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72dd3ac10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),                  # Cross-Entropy loss function\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 0.0001),            #Adam optimizer\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "model3.fit(flatxtrain, y_train, batch_size =512, epochs =100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "364b61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2336 - accuracy: 0.9464\n",
      "accuracy = 94.6399986743927 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model on test set\n",
    "(h3,acc3) = model3.evaluate(flatxtest, y_test)\n",
    "print('accuracy =', acc3*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ad904c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd734b47a00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e8f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(flatxtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "383f765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = [np.argmax(i) for i in y_pred]\n",
    "y_predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b6a1cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1752855",
   "metadata": {},
   "source": [
    "## Part 2: Backpropagation\n",
    "### Backpropagation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac00ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from joblib import Memory\n",
    "memory = Memory('./tmp')\n",
    "fetch_openml_cached = memory.cache(fetch_openml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbc9c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.datasets._openml.fetch_openml...\n",
      "fetch_openml('mnist_784', version=1, cache=True, return_X_y=True, as_frame=False)\n",
      "____________________________________________________fetch_openml - 27.2s, 0.5min\n"
     ]
    }
   ],
   "source": [
    "#Fetching the mnist dataset and slpitting the data into testa and train\n",
    "x, y = fetch_openml_cached('mnist_784', version = 1, cache = True, return_X_y = True, as_frame = False)\n",
    "\n",
    "x = (x / 255).astype('float32')\n",
    "y = to_categorical(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593c3e9",
   "metadata": {},
   "source": [
    "Next we initialise the network by assigning random values to weights. We are would be using a neural net 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "967e81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation\n",
    "def nn_init(lsize):\n",
    "    # renaming for clarity\n",
    "    input_layer  = lsize[0]\n",
    "    hidden_1     = lsize[1]\n",
    "    hidden_2     = lsize[2]\n",
    "    output_layer = lsize[3]\n",
    "    nnet = {\n",
    "        'w0': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "        'w1': np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "        'w2': np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "    }\n",
    "    return nnet\n",
    "\n",
    "layer_sizes = [x_train[0].shape[0], 128, 64, y_train[0].shape[0]]\n",
    "model = nn_init(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfecdad",
   "metadata": {},
   "source": [
    "Defining the functions required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cdcc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(x, derivative = False):\n",
    "    if derivative:\n",
    "        return np.exp(-x) / ((np.exp(-x) + 1) ** 2)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20043187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax function\n",
    "def softmax(x, derivative = False):\n",
    "\n",
    "    exp_shifted = np.exp(x - x.max())\n",
    "    if derivative:\n",
    "        return exp_shifted / np.sum(exp_shifted, axis = 0) * (1 - exp_shifted / np.sum(exp_shifted, axis = 0))\n",
    "    else:\n",
    "        return exp_shifted / np.sum(exp_shifted, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daf388f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "\n",
    "def forward_pass(x):\n",
    "    nn_state = {}\n",
    "    nn_state['o0'] = x\n",
    "    # weighted sum of all activations, then using the sigmoid function\n",
    "    nn_state['z1'] = np.dot(model['w0'], nn_state['o0'])\n",
    "    nn_state['o1'] = sigmoid(nn_state['z1'])\n",
    "    \n",
    "    # from hidden 1 to hidden 2\n",
    "    nn_state['z2'] = np.dot(model['w1'], nn_state['o1'])\n",
    "    nn_state['o2'] = sigmoid(nn_state['z2'])\n",
    "    \n",
    "    # from hidden 2 to output\n",
    "    nn_state['z3'] = np.dot(model['w2'], nn_state['o2'])\n",
    "    nn_state['o3'] = softmax(nn_state['z3'])\n",
    "    \n",
    "    return nn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2df88",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ab100",
   "metadata": {},
   "source": [
    "We know for a sigmoid function, the cost function is given as  C = -Y.log(O) - (1-Y).log(1-O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fcad06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def part_cost(o, y):\n",
    "    c = np.dot(y, np.log(o)) + np.dot((1 - y), np.log(1 - o))\n",
    "    return -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cada2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "def backward_pass(x, y):\n",
    "    # doing the forward pass and registering the state of the network\n",
    "    nn_state = forward_pass(x)\n",
    "    \n",
    "    # derivatives of the error\n",
    "    nn_state['d3'] = nn_state['o3'] - y\n",
    "    nn_state['d2'] = np.dot(nn_state['d3'], model['w2']) * softmax(nn_state['z2'], derivative = True)\n",
    "    nn_state['d1'] = np.dot(nn_state['d2'], model['w1']) * sigmoid(nn_state['z1'], derivative = True)\n",
    "    \n",
    "    # adjustments to weights\n",
    "    nn_state['D2'] = np.outer(nn_state['d3'], nn_state['o2'])\n",
    "    nn_state['D1'] = np.outer(nn_state['d2'], nn_state['o1'])\n",
    "    nn_state['D0'] = np.outer(nn_state['d1'], nn_state['o0'])\n",
    "    \n",
    "    return nn_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05920e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================TRAINING=============================\n",
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21701c2f2d104a41b459d7d43195f264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.947620762116127 accuracy: 0.7025102040816327\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e207f42baeb64b35a6621801464e9089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.8634751822277376 accuracy: 0.7121428571428572\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ca2cb1394b4b508f481f64a2480b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.7996459626807058 accuracy: 0.7207346938775511\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483445160b434c88859c80d4d4105050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.7575075172861894 accuracy: 0.7280408163265306\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2514f3c925594d1d865b2d326f257786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.7781371970895354 accuracy: 0.7296734693877551\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "epochs = 5\n",
    "t_rate = 0.001\n",
    "batch = 32\n",
    "# train\n",
    "print('=============================TRAINING=============================')\n",
    "for e in range(epochs):\n",
    "    print('epoch:', e)\n",
    "    \n",
    "    samples = x_train.shape[0]\n",
    "    cost = 0\n",
    "    cnt = 0\n",
    "    for i in tqdm(range(samples)):\n",
    "        m_state = backward_pass(x_train[i], y_train[i])\n",
    "       # Using mini batch gradient descent\n",
    "        cost += part_cost(m_state['o3'], y_train[i])\n",
    "        # update weights\n",
    "        model['w0'] -= t_rate * m_state['D0']\n",
    "        model['w1'] -= t_rate * m_state['D1']\n",
    "        model['w2'] -= t_rate * m_state['D2']\n",
    "        \n",
    "        if np.argmax(m_state['o3']) == np.argmax(y_train[i]):\n",
    "            # successful detection\n",
    "            cnt += 1\n",
    "# performance evaluation\n",
    "    cost = cost / samples\n",
    "    accuracy = cnt / samples\n",
    "    print('cost:', cost, 'accuracy:', accuracy)\n",
    "#'''\n",
    "# save the model\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71ca4b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++ testing +++++++++++++++++++++++++++\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b5ec7b6a2481c865508ca7c50c6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.793773351158152 accuracy: 0.7406666666666667\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "print('+++++++++++++++++++++++++++ testing +++++++++++++++++++++++++++')\n",
    "# load the model\n",
    "if os.path.isfile('model.pickle'):\n",
    "    with open('model.pickle', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "# run the whole test data\n",
    "samples = x_test.shape[0]\n",
    "cost = 0\n",
    "hit_count = 0\n",
    "for i in tqdm(range(samples)):\n",
    "    m_state = forward_pass(x_test[i])\n",
    "    cost += part_cost(m_state['o3'], y_test[i])\n",
    "    if np.argmax(m_state['o3']) == np.argmax(y_test[i]):\n",
    "        hit_count += 1\n",
    "# evaluate performance\n",
    "cost = cost / samples\n",
    "acc_backprop = hit_count / samples\n",
    "print('cost:', cost, 'accuracy:', acc_backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3cb44",
   "metadata": {},
   "source": [
    "##  Using standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd20bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d257271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset(mnist)\n",
    "from keras.datasets import mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87226052",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestrain=[]\n",
    "featurestest=[]\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "    featurestrain.append(np.reshape(train_X[i], (28*28)))\n",
    "for i in range(len(test_X)):\n",
    "    featurestest.append(np.reshape(test_X[i], (28*28)))\n",
    "\n",
    "featurestrain=np.array(featurestrain)\n",
    "featurestest=np.array(featurestest)\n",
    "featurestrain=featurestrain.astype(\"float32\")\n",
    "featurestest=featurestest.astype(\"float32\")\n",
    "\n",
    "# Scaling\n",
    "for i in range(len(train_X)):\n",
    "    featurestrain[i]=featurestrain[i]/255.0\n",
    "for i in range(len(test_X)):\n",
    "    featurestest[i]=featurestest[i]/255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adac177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts dataset labels vector to categorical data matrix: \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train_y, 10)\n",
    "y_test = to_categorical(test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e9d38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model1 = keras.Sequential(name=\"\")\n",
    "model1.add(keras.layers.InputLayer((784)))\n",
    "model1.add(layers.Dense(128, activation=\"sigmoid\", name=\"layer1\"))\n",
    "model1.add(layers.Dense(64, activation=\"sigmoid\", name=\"layer2\"))\n",
    "model1.add(layers.Dense(10, activation=\"softmax\", name=\"outputlayer\"))\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f560f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0115 - accuracy: 0.9821\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0097 - accuracy: 0.9852\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0086 - accuracy: 0.9872\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0074 - accuracy: 0.9894\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0062 - accuracy: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72f604940>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "model1.fit(featurestrain,y_train,epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55b5b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9785000085830688"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating performance on the test set of our tensorflow model()\n",
    "(h4,acc_stdlib) =model1.evaluate(featurestest,y_test)\n",
    "acc_stdlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82c3e6",
   "metadata": {},
   "source": [
    "## Writing the final results in a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24a9b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('results.txt','w')\n",
    "file.write(\"\\t\\t\\tFINAL RESULTS\\n\\t\\t       MLFA Assignment-3\\n--------------------------------------------------------------\\n--------------------------------------------------------------\\n\")\n",
    "file.write(\"\\nPART-1: Effect of Neural Network Configuration\")\n",
    "str1 = \"\\nAccuracy for UniformNet = \" + str(acc1*100) + \" % \"\n",
    "file.write(str1)\n",
    "str2 = \"\\nAccuracy for PyramidNet = \" + str(acc2*100) + \" % \"\n",
    "file.write(str2)\n",
    "str3 = \"\\nAccuracy for InvPyramidNet = \" + str(acc3*100) + \" % \"\n",
    "file.write(str3)\n",
    "file.write(\"\\n--------------------------------------------------------------\\n--------------------------------------------------------------\\n\")\n",
    "file.write(\"\\nPART-2: Backpropagation\")\n",
    "str4 = \"\\nAccuracy for code written from scratch = \" + str(acc_backprop*100) + \" % \"\n",
    "file.write(str4)\n",
    "str4 = \"\\nAccuracy by using standard libraries \" + str(acc_stdlib*100) + \" % \"\n",
    "file.write(str4)\n",
    "file.write(\"\\n--------------------------------------------------------------\\n--------------------------------------------------------------\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdaea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
